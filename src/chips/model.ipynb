{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rebal\\capstone-project\\capstone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, Adagrad\n",
    "from torch_geometric.utils import degree\n",
    "import pickle\n",
    "from torch import optim\n",
    "\n",
    "# For visualization\n",
    "# from experiments.cross_design.utils import *\n",
    "\n",
    "# PyTorch data loader\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# PyTorch geometric data loader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pyg_dataset import pyg_dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "# For Laplacian position encoding\n",
    "from scipy.sparse import csgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.ArgumentParser(description = 'Supervised learning')\n",
    "args.dir = './train_gnn_hetero/demand'\n",
    "args.target = 'demand'\n",
    "args.data_dir = '../../data/chips/clean_data/'\n",
    "args.name = 'test'\n",
    "args.num_epoch = 10\n",
    "args.batch_size = 1\n",
    "args.learning_rate = 0.001\n",
    "args.seed = 123456789\n",
    "args.n_layers = 3\n",
    "args.hidden_dim = 20\n",
    "args.test_mode = 0\n",
    "args.pe = 'lap'\n",
    "args.pos_dim = 10\n",
    "args.virtual_node = 0\n",
    "args.gnn_type = 'gcn'\n",
    "args.load_global_info = 0\n",
    "args.load_pd = False\n",
    "args.fold = 0\n",
    "args.device = 'cpu'\n",
    "args.design = 19\n",
    "args.pl = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from gnn_hetero import GNN\n",
    "from yacs.config import CfgNode as CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.test_mode == 0:\n",
    "    log_name = args.dir + \"/\" + args.name + \".log\"\n",
    "else:\n",
    "    print(\"Test mode\")\n",
    "    log_name = args.dir + \"/\" + args.name + \".test_mode.log\"\n",
    "model_name = args.dir + \"/\" + args.name + \".model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Fix CPU torch random seed\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Fix GPU torch random seed\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Fix the Numpy random seed\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "# device = 'cpu'\n",
    "device = args.device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create data loaders: concat True, sparse False, gnntype gcn\n"
     ]
    }
   ],
   "source": [
    "if args.gnn_type not in [\"gcn\", \"gat\"]:\n",
    "    sparse = True\n",
    "else:\n",
    "    sparse = False\n",
    "\n",
    "if args.virtual_node >= 1:\n",
    "    virtual_node = True\n",
    "    single = False\n",
    "    if args.virtual_node == 2:\n",
    "        single = True\n",
    "else:\n",
    "    single = False\n",
    "    virtual_node = False\n",
    "\n",
    "if sparse:\n",
    "    from pyg_dataset_sparse import *\n",
    "    concat = True\n",
    "else:\n",
    "    concat = True\n",
    "\n",
    "print(f'Create data loaders: concat {concat}, sparse {sparse}, gnntype {args.gnn_type}')\n",
    "pe = args.pe\n",
    "pos_dim = args.pos_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = None\n",
    "use_signnet = False\n",
    "if pe == 'signnet':\n",
    "    use_signnet = True\n",
    "    config = CN()\n",
    "    config = set_cfg_posenc(config)\n",
    "    config.posenc_SignNet.model = 'DeepSet'\n",
    "    config.posenc_SignNet.post_layers = 2\n",
    "    config.posenc_SignNet.dim_pe = pos_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/chips/clean_data/\n"
     ]
    }
   ],
   "source": [
    "print(args.data_dir)\n",
    "\n",
    "load_global_info = False\n",
    "if args.load_global_info == 1:\n",
    "    load_global_info = True\n",
    "\n",
    "load_pd = False\n",
    "if args.load_pd == 1:\n",
    "    load_pd = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning target: demand\n",
      "Number of samples: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 196.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([3952, 6])\n",
      "torch.Size([3952, 4])\n",
      "torch.Size([3952, 4])\n",
      "1\n",
      "torch.Size([6872, 6])\n",
      "torch.Size([6872, 4])\n",
      "torch.Size([6872, 4])\n",
      "2\n",
      "torch.Size([6913, 6])\n",
      "torch.Size([6913, 4])\n",
      "torch.Size([6913, 4])\n",
      "3\n",
      "torch.Size([7323, 6])\n",
      "torch.Size([7323, 4])\n",
      "torch.Size([7323, 4])\n",
      "4\n",
      "torch.Size([7258, 6])\n",
      "torch.Size([7258, 4])\n",
      "torch.Size([7258, 4])\n",
      "5\n",
      "torch.Size([7120, 6])\n",
      "torch.Size([7120, 4])\n",
      "torch.Size([7120, 4])\n",
      "6\n",
      "torch.Size([7879, 6])\n",
      "torch.Size([7879, 4])\n",
      "torch.Size([7879, 4])\n",
      "7\n",
      "torch.Size([7626, 6])\n",
      "torch.Size([7626, 4])\n",
      "torch.Size([7626, 4])\n",
      "8\n",
      "torch.Size([7620, 6])\n",
      "torch.Size([7620, 4])\n",
      "torch.Size([7620, 4])\n",
      "9\n",
      "torch.Size([7772, 6])\n",
      "torch.Size([7772, 4])\n",
      "torch.Size([7772, 4])\n",
      "10\n",
      "torch.Size([7814, 6])\n",
      "torch.Size([7814, 4])\n",
      "torch.Size([7814, 4])\n",
      "Done reading data\n",
      "Learning target: demand\n",
      "Number of samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 278.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "torch.Size([6529, 6])\n",
      "torch.Size([6529, 4])\n",
      "torch.Size([6529, 4])\n",
      "Done reading data\n",
      "Learning target: demand\n",
      "Number of samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 332.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([6548, 6])\n",
      "torch.Size([6548, 4])\n",
      "torch.Size([6548, 4])\n",
      "Done reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if pe == 'lap':\n",
    "    train_dataset = pyg_dataset(design = args.design, pl = args.pl, data_dir = args.data_dir, fold_index = args.fold, split = 'train', target = args.target, load_pe = True, num_eigen = pos_dim, load_global_info = load_global_info, load_pd = load_pd, vn = virtual_node, concat = concat, net = False)\n",
    "    valid_dataset = pyg_dataset(design = args.design, pl = args.pl, data_dir = args.data_dir, fold_index = args.fold, split = 'valid', target = args.target, load_pe = True, num_eigen = pos_dim, load_global_info = load_global_info, load_pd = load_pd, vn = virtual_node, concat = concat, net = False)\n",
    "    test_dataset = pyg_dataset(design = args.design, pl = args.pl, data_dir = args.data_dir, fold_index = args.fold, split = 'test', target = args.target, load_pe = True, num_eigen = pos_dim, load_global_info = load_global_info, load_pd = load_pd, vn = virtual_node, concat = concat, net = False)\n",
    "else:\n",
    "    train_dataset = pyg_dataset(data_dir = args.data_dir, fold_index = args.fold, design = args.design, pl = args.pl, split = 'train', target = args.target, load_global_info = load_global_info, load_pd = load_pd, load_pe = False, vn=args.virtual_node, concat = concat)\n",
    "    valid_dataset = pyg_dataset(data_dir = args.data_dir, fold_index = args.fold, design = args.design, pl = args.pl, split = 'valid', target = args.target, load_global_info = load_global_info, load_pd = load_pd, load_pe = False, vn = args.virtual_node, concat = concat)\n",
    "    test_dataset = pyg_dataset(data_dir = args.data_dir, fold_index = args.fold, design = args.design, pl = args.pl, split = 'test', target = args.target, load_global_info = load_global_info, load_pd = load_pd, load_pe = False, vn=args.virtual_node, concat = concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[8434, 5], num_instances=3952, y=[3952, 1, 13], edge_index_node_net=[2, 17444], edge_index_net_node=[2, 17444], cell_degrees=[3952], net_degrees=[4482], evects=[8434, 10])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.data_dir + '0.eigen.10.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8434, 10)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['evects'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.data_dir + '0.node_features.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_instances': 3952,\n",
       " 'num_nets': 4482,\n",
       " 'x_min': 512,\n",
       " 'x_max': 84096,\n",
       " 'y_min': 1536,\n",
       " 'y_max': 87552,\n",
       " 'min_cell_width': 256,\n",
       " 'max_cell_width': 397440,\n",
       " 'min_cell_height': 1536,\n",
       " 'max_cell_height': 503056,\n",
       " 'instance_features': array([[4.96171516e-01, 5.00000000e-01, 2.30000000e+01, 4.83403158e-03,\n",
       "         0.00000000e+00, 0.00000000e+00],\n",
       "        [4.96171516e-01, 5.35714286e-01, 2.30000000e+01, 4.83403158e-03,\n",
       "         0.00000000e+00, 6.00000000e+00],\n",
       "        [5.22205207e-01, 5.00000000e-01, 2.30000000e+01, 4.83403158e-03,\n",
       "         0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.72588055e-01, 7.50000000e-01, 3.40000000e+01, 3.22268772e-04,\n",
       "         0.00000000e+00, 0.00000000e+00],\n",
       "        [4.79326187e-01, 5.00000000e-01, 1.10000000e+01, 9.66806316e-04,\n",
       "         0.00000000e+00, 0.00000000e+00],\n",
       "        [5.54364472e-01, 5.00000000e-01, 1.10000000e+01, 9.66806316e-04,\n",
       "         0.00000000e+00, 6.00000000e+00]]),\n",
       " 'sample_name': '../../data/chips/NCSU-DigIC-GraphData-2023-07-25/xbar/13/',\n",
       " 'folder': '../../data/chips/NCSU-DigIC-GraphData-2023-07-25/xbar/1/',\n",
       " 'design': 'xbar'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of training examples: 11\n",
      "Number of testing examples: 1\n"
     ]
    }
   ],
   "source": [
    "batch_size = args.batch_size\n",
    "print(batch_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size, shuffle = False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle = False)\n",
    "\n",
    "print('Number of training examples:', len(train_dataset))\n",
    "print('Number of testing examples:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "DataBatch(x=[15784, 5], num_instances=[1], y=[7626, 1, 13], edge_index_node_net=[2, 25645], edge_index_net_node=[2, 25645], cell_degrees=[7626], net_degrees=[8158], evects=[15784, 10], batch=[15784], ptr=[2])\n",
      "1\n",
      "DataBatch(x=[16160, 5], num_instances=[1], y=[7814, 1, 13], edge_index_node_net=[2, 26023], edge_index_net_node=[2, 26023], cell_degrees=[7814], net_degrees=[8346], evects=[16160, 10], batch=[16160], ptr=[2])\n",
      "2\n",
      "DataBatch(x=[15178, 5], num_instances=[1], y=[7323, 1, 13], edge_index_node_net=[2, 25033], edge_index_net_node=[2, 25033], cell_degrees=[7323], net_degrees=[7855], evects=[15178, 10], batch=[15178], ptr=[2])\n",
      "3\n",
      "DataBatch(x=[8434, 5], num_instances=[1], y=[3952, 1, 13], edge_index_node_net=[2, 17444], edge_index_net_node=[2, 17444], cell_degrees=[3952], net_degrees=[4482], evects=[8434, 10], batch=[8434], ptr=[2])\n",
      "4\n",
      "DataBatch(x=[16076, 5], num_instances=[1], y=[7772, 1, 13], edge_index_node_net=[2, 25926], edge_index_net_node=[2, 25926], cell_degrees=[7772], net_degrees=[8304], evects=[16076, 10], batch=[16076], ptr=[2])\n",
      "5\n",
      "DataBatch(x=[14276, 5], num_instances=[1], y=[6872, 1, 13], edge_index_node_net=[2, 24100], edge_index_net_node=[2, 24100], cell_degrees=[6872], net_degrees=[7404], evects=[14276, 10], batch=[14276], ptr=[2])\n",
      "6\n",
      "DataBatch(x=[14358, 5], num_instances=[1], y=[6913, 1, 13], edge_index_node_net=[2, 24189], edge_index_net_node=[2, 24189], cell_degrees=[6913], net_degrees=[7445], evects=[14358, 10], batch=[14358], ptr=[2])\n",
      "7\n",
      "DataBatch(x=[14772, 5], num_instances=[1], y=[7120, 1, 13], edge_index_node_net=[2, 24631], edge_index_net_node=[2, 24631], cell_degrees=[7120], net_degrees=[7652], evects=[14772, 10], batch=[14772], ptr=[2])\n",
      "8\n",
      "DataBatch(x=[15048, 5], num_instances=[1], y=[7258, 1, 13], edge_index_node_net=[2, 24894], edge_index_net_node=[2, 24894], cell_degrees=[7258], net_degrees=[7790], evects=[15048, 10], batch=[15048], ptr=[2])\n",
      "9\n",
      "DataBatch(x=[16290, 5], num_instances=[1], y=[7879, 1, 13], edge_index_node_net=[2, 26179], edge_index_net_node=[2, 26179], cell_degrees=[7879], net_degrees=[8411], evects=[16290, 10], batch=[16290], ptr=[2])\n",
      "10\n",
      "DataBatch(x=[15773, 5], num_instances=[1], y=[7620, 1, 13], edge_index_node_net=[2, 25635], edge_index_net_node=[2, 25635], cell_degrees=[7620], net_degrees=[8153], evects=[15773, 10], batch=[15773], ptr=[2])\n",
      "Number of node features: 5\n",
      "Number of edge features: 1\n",
      "Number of outputs: 1\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    print(batch_idx)\n",
    "    print(data)\n",
    "    node_dim = data.x.size(1)\n",
    "    edge_dim = 1\n",
    "    # if sparse:\n",
    "    #     edge_dim = 1\n",
    "    #     #net_dim = data.x_net.size(1)\n",
    "    # else:\n",
    "    #     edge_dim = data.edge_attr.size(1)\n",
    "    #     #net_dim = data.x_net.size(1)\n",
    "        \n",
    "    if args.target == 'classify':\n",
    "        num_outputs = 2#data.y.size(1)\n",
    "    else:\n",
    "        num_outputs = data.y.size(1)\n",
    "\n",
    "\n",
    "print('Number of node features:', node_dim)\n",
    "print('Number of edge features:', edge_dim)\n",
    "print('Number of outputs:', num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eigenvectors: 10\n",
      "Number of node features + position encoding: 15\n"
     ]
    }
   ],
   "source": [
    "if pe == 'lap':\n",
    "    node_dim += pos_dim\n",
    "\n",
    "    print('Number of eigenvectors:', pos_dim)\n",
    "    print('Number of node features + position encoding:', node_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y min: 0.0\n",
      "y max: 34.0\n",
      "y mean: 2.083835\n",
      "y std: 3.6699028\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    y.append(data.y.detach().numpy())\n",
    "y = np.concatenate(y)\n",
    "\n",
    "y_min = np.min(y)\n",
    "y_max = np.max(y)\n",
    "y_mean = np.mean(y)\n",
    "y_std = np.std(y)\n",
    "\n",
    "print('y min:', y_min)\n",
    "print('y max:', y_max)\n",
    "print('y mean:', y_mean)\n",
    "print('y std:', y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN type: gcn\n",
      "Virtual node: False\n"
     ]
    }
   ],
   "source": [
    "if args.virtual_node == 1:\n",
    "    virtual_node = True\n",
    "else:\n",
    "    virtual_node = False\n",
    "gnn_type = args.gnn_type\n",
    "\n",
    "print('GNN type:', gnn_type)\n",
    "print('Virtual node:', virtual_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN_node(\n",
      "  (node_encoder): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=40, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.1)\n",
      "    (2): Linear(in_features=40, out_features=20, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv()\n",
      "    (1): GCNConv()\n",
      "    (2): GCNConv()\n",
      "  )\n",
      "  (re_convs): ModuleList(\n",
      "    (0): GCNConv()\n",
      "    (1): GCNConv()\n",
      "    (2): GCNConv()\n",
      "  )\n",
      "  (norms): ModuleList(\n",
      "    (0): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if gnn_type == 'pna':\n",
    "    aggregators = ['mean', 'min', 'max', 'std']\n",
    "    scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "    print('Computing the in-degree histogram')\n",
    "    deg = torch.zeros(10, dtype = torch.long)\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        d = degree(data.edge_index[1], num_nodes = data.num_nodes, dtype = torch.long)\n",
    "        deg += torch.bincount(d, minlength = deg.numel())\n",
    "    print('Done computing the in-degree histogram')\n",
    "\n",
    "    model = GNN(gnn_type = gnn_type, num_tasks = num_outputs, virtual_node = virtual_node, num_layer = args.n_layers, emb_dim = args.hidden_dim,\n",
    "            aggregators = aggregators, scalers = scalers, deg = deg, edge_dim = edge_dim, \n",
    "            use_signnet = use_signnet, node_dim = node_dim, cfg_posenc = config,\n",
    "            device = device, single = single).to(device)\n",
    "else:\n",
    "    model = GNN(gnn_type = gnn_type, num_tasks = num_outputs, virtual_node = virtual_node, num_layer = args.n_layers, emb_dim = args.hidden_dim,\n",
    "            use_signnet = use_signnet, node_dim = node_dim, edge_dim = edge_dim, cfg_posenc = config,\n",
    "            device = device, single = single).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learnable parameters: 46205\n",
      "Done with model creation\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adagrad(model.parameters(), lr = args.learning_rate)\n",
    "num_parameters = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print('Number of learnable parameters:', num_parameters)\n",
    "print('Done with model creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.test_mode == 1:\n",
    "    print(\"Skip the training\")\n",
    "    num_epoch = 0\n",
    "else:\n",
    "    num_epoch = args.num_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rebal\\AppData\\Local\\Temp\\ipykernel_9716\\2570391028.py:48: UserWarning: Using a target size (torch.Size([89869])) that is different to the input size (torch.Size([6913])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(predict.view(-1), target.view(-1), reduction = 'mean')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6913) must match the size of tensor b (89869) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(predict, target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\rebal\\capstone-project\\capstone\\lib\\site-packages\\torch\\nn\\functional.py:3291\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3289\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3291\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\rebal\\capstone-project\\capstone\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (6913) must match the size of tensor b (89869) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "best_mae = 1e9\n",
    "patience = 300\n",
    "stop = False\n",
    "for epoch in range(num_epoch):\n",
    "    if stop:\n",
    "        break\n",
    "    print('--------------------------------------')\n",
    "    print('Epoch', epoch)\n",
    "    # LOG.write('--------------------------------------\\n')\n",
    "    # LOG.write('Epoch ' + str(epoch) + '\\n')\n",
    "\n",
    "    # Training\n",
    "    t = time.time()\n",
    "    total_loss = 0.0\n",
    "    nBatch = 0\n",
    "    sum_error = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        data = data.to(device = device)\n",
    "        if args.target == 'classify':\n",
    "            target = data.y\n",
    "        else:\n",
    "            target = (data.y - y_mean) / y_std\n",
    "        #weights = data.weights.to(device = device)\n",
    "\n",
    "        if pe == 'lap':\n",
    "            data.x = torch.cat([data.x, data.evects], dim = 1)\n",
    "            #print(data.x.shape, data.evects.shape)\n",
    "\n",
    "        if use_signnet == True:\n",
    "            data.x = data.x.type(torch.FloatTensor).to(device = device)\n",
    "\n",
    "        if gnn_type == 'pna':\n",
    "            data.edge_attr = data.edge_attr.type(torch.FloatTensor).to(device = device)\n",
    "        \n",
    "        #print(data.x.shape)\n",
    "        \n",
    "        predict = model(data)\n",
    "        predict = predict[: target.size(0), :]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mean squared error loss\n",
    "        if args.target == 'classify':\n",
    "            loss = F.nll_loss(predict, target.view(-1))\n",
    "        else:\n",
    "            loss = F.mse_loss(predict.view(-1), target.view(-1), reduction = 'mean')\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        nBatch += 1\n",
    "\n",
    "        if args.target == 'classify':\n",
    "            sum_error += loss.item()\n",
    "        else:\n",
    "            sum_error += torch.sum(torch.abs(predict.view(-1) - target.view(-1))).detach().cpu().numpy()\n",
    "        num_samples += predict.size(0)\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Batch', batch_idx, '/', len(train_dataloader),': Loss =', loss.item())\n",
    "            # LOG.write('Batch ' + str(batch_idx) + '/' + str(len(train_dataloader)) + ': Loss = ' + str(loss.item()) + '\\n')\n",
    "\n",
    "    train_mae = sum_error / (num_samples * num_outputs)\n",
    "    avg_loss = total_loss / nBatch\n",
    "\n",
    "    print('Train average loss:', avg_loss)\n",
    "    # LOG.write('Train average loss: ' + str(avg_loss) + '\\n')\n",
    "    print('Train MAE:', train_mae)\n",
    "    # LOG.write('Train MAE: ' + str(train_mae) + '\\n')\n",
    "    print('Train MAE (original scale):', train_mae * y_std)\n",
    "    # LOG.write('Train MAE (original scale): ' + str(train_mae * y_std) + '\\n')\n",
    "    print(\"Train time =\", \"{:.5f}\".format(time.time() - t))\n",
    "    # LOG.write(\"Train time = \" + \"{:.5f}\".format(time.time() - t) + \"\\n\")\n",
    "\n",
    "    # Validation\n",
    "    t = time.time()\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    nBatch = 0\n",
    " \n",
    "    with torch.no_grad():\n",
    "        sum_error = 0.0\n",
    "        num_samples = 0\n",
    "        for batch_idx, data in enumerate(valid_dataloader):\n",
    "            data = data.to(device = device)\n",
    "            if args.target == 'classify':\n",
    "                target = data.y\n",
    "            else:\n",
    "                target = (data.y - y_mean) / y_std\n",
    "\n",
    "            if pe == 'lap':\n",
    "                data.x = torch.cat([data.x, data.evects], dim = 1)\n",
    "\n",
    "            if use_signnet == True:\n",
    "                data.x = data.x.type(torch.FloatTensor).to(device = device)\n",
    "\n",
    "            if gnn_type == 'pna':\n",
    "                data.edge_attr = data.edge_attr.type(torch.FloatTensor).to(device = device)\n",
    "\n",
    "            predict = model(data)\n",
    "            predict = predict[: target.size(0), :]\n",
    "\n",
    "            # Mean squared error loss\n",
    "            if args.target == 'classify':\n",
    "                loss = F.nll_loss(predict, target.view(-1))\n",
    "            else:\n",
    "                loss = F.mse_loss(predict.view(-1), target.view(-1), reduction = 'mean')\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            nBatch += 1\n",
    "\n",
    "            if args.target == 'classify':\n",
    "                sum_error += loss.item()\n",
    "            else:\n",
    "                sum_error += torch.sum(torch.abs(predict.view(-1) - target.view(-1))).detach().cpu().numpy()\n",
    "            num_samples += predict.size(0)\n",
    "             \n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Valid Batch', batch_idx, '/', len(valid_dataloader),': Loss =', loss.item())\n",
    "                # LOG.write('Valid Batch ' + str(batch_idx) + '/' + str(len(valid_dataloader)) + ': Loss = ' + str(loss.item()) + '\\n')\n",
    "\n",
    "    valid_mae = sum_error / (num_samples * num_outputs)\n",
    "    avg_loss = total_loss / nBatch\n",
    "\n",
    "    print('Valid average loss:', avg_loss)\n",
    "    # LOG.write('Valid average loss: ' + str(avg_loss) + '\\n')\n",
    "    print('Valid MAE:', valid_mae)\n",
    "    # LOG.write('Valid MAE: ' + str(valid_mae) + '\\n')\n",
    "    print('Valid MAE (original scale):', valid_mae * y_std)\n",
    "    # LOG.write('Valid MAE (original scale): ' + str(valid_mae * y_std) + '\\n')\n",
    "    print(\"Valid time =\", \"{:.5f}\".format(time.time() - t))\n",
    "    # LOG.write(\"Valid time = \" + \"{:.5f}\".format(time.time() - t) + \"\\n\")\n",
    "    \n",
    "    if valid_mae < best_mae:\n",
    "        best_mae = valid_mae\n",
    "        patience = 300\n",
    "        print('Current best MAE updated:', best_mae)\n",
    "        # LOG.write('Current best MAE updated: ' + str(best_mae) + '\\n')\n",
    "        print('Current best MAE (original scale) updated:', best_mae * y_std)\n",
    "        # LOG.write('Current best MAE (original scale) updated: ' + str(best_mae * y_std) + '\\n')\n",
    "        \n",
    "        torch.save(model.state_dict(), model_name)\n",
    "        print(\"Save the best model to \" + model_name)\n",
    "        # LOG.write(\"Save the best model to \" + model_name + \"\\n\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        print(f'Patience: {patience}')\n",
    "        if patience <= 0:\n",
    "            stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
