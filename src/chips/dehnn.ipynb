{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "from scipy.sparse import coo_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 15\n",
      "15 20\n",
      "20 30\n",
      "30 40\n",
      "tensor([[0.5437, 0.4563],\n",
      "        [0.5437, 0.4563],\n",
      "        [0.5850, 0.4150],\n",
      "        [0.5751, 0.4249]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.5198, 0.4802],\n",
      "        [0.5198, 0.4802],\n",
      "        [0.5462, 0.4538],\n",
      "        [0.5593, 0.4407]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.4987, 0.5013],\n",
      "        [0.4987, 0.5013],\n",
      "        [0.5105, 0.4895],\n",
      "        [0.5445, 0.4555]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.4996, 0.5004],\n",
      "        [0.4996, 0.5004],\n",
      "        [0.5192, 0.4808],\n",
      "        [0.5523, 0.4477]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5301, 0.4699],\n",
      "        [0.5629, 0.4371]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.4978, 0.5022],\n",
      "        [0.4978, 0.5022],\n",
      "        [0.5356, 0.4644],\n",
      "        [0.5704, 0.4296]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor([[0.4965, 0.5035],\n",
      "        [0.4965, 0.5035],\n",
      "        [0.5347, 0.4653],\n",
      "        [0.5710, 0.4290]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.4949, 0.5051],\n",
      "        [0.4949, 0.5051],\n",
      "        [0.5372, 0.4628],\n",
      "        [0.5751, 0.4249]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.4870, 0.5130],\n",
      "        [0.4870, 0.5130],\n",
      "        [0.5264, 0.4736],\n",
      "        [0.5720, 0.4280]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.4853, 0.5147],\n",
      "        [0.4853, 0.5147],\n",
      "        [0.5237, 0.4763],\n",
      "        [0.5711, 0.4289]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "Epoch [10/100], Loss: 0.7057\n",
      "tensor([[0.4852, 0.5148],\n",
      "        [0.4852, 0.5148],\n",
      "        [0.5233, 0.4767],\n",
      "        [0.5707, 0.4293]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.4848, 0.5152],\n",
      "        [0.4848, 0.5152],\n",
      "        [0.5254, 0.4746],\n",
      "        [0.5731, 0.4269]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.4828, 0.5172],\n",
      "        [0.4828, 0.5172],\n",
      "        [0.5250, 0.4750],\n",
      "        [0.5745, 0.4255]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.4807, 0.5193],\n",
      "        [0.4807, 0.5193],\n",
      "        [0.5269, 0.4731],\n",
      "        [0.5783, 0.4217]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.4773, 0.5227],\n",
      "        [0.4773, 0.5227],\n",
      "        [0.5262, 0.4738],\n",
      "        [0.5810, 0.4190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.4726, 0.5274],\n",
      "        [0.4726, 0.5274],\n",
      "        [0.5233, 0.4767],\n",
      "        [0.5826, 0.4174]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.4711, 0.5289],\n",
      "        [0.4711, 0.5289],\n",
      "        [0.5270, 0.4730],\n",
      "        [0.5877, 0.4123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.4712, 0.5288],\n",
      "        [0.4712, 0.5288],\n",
      "        [0.5324, 0.4676],\n",
      "        [0.5928, 0.4072]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.4710, 0.5290],\n",
      "        [0.4710, 0.5290],\n",
      "        [0.5393, 0.4607],\n",
      "        [0.5997, 0.4003]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.4651, 0.5349],\n",
      "        [0.4651, 0.5349],\n",
      "        [0.5348, 0.4652],\n",
      "        [0.6009, 0.3991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "Epoch [20/100], Loss: 0.6610\n",
      "tensor([[0.4622, 0.5378],\n",
      "        [0.4622, 0.5378],\n",
      "        [0.5364, 0.4636],\n",
      "        [0.6052, 0.3948]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.4612, 0.5388],\n",
      "        [0.4612, 0.5388],\n",
      "        [0.5398, 0.4602],\n",
      "        [0.6094, 0.3906]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.4577, 0.5423],\n",
      "        [0.4577, 0.5423],\n",
      "        [0.5365, 0.4635],\n",
      "        [0.6095, 0.3905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.4529, 0.5471],\n",
      "        [0.4529, 0.5471],\n",
      "        [0.5313, 0.4687],\n",
      "        [0.6089, 0.3911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.4502, 0.5498],\n",
      "        [0.4502, 0.5498],\n",
      "        [0.5284, 0.4716],\n",
      "        [0.6086, 0.3914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.4496, 0.5504],\n",
      "        [0.4496, 0.5504],\n",
      "        [0.5279, 0.4721],\n",
      "        [0.6087, 0.3913]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.4507, 0.5493],\n",
      "        [0.4507, 0.5493],\n",
      "        [0.5294, 0.4706],\n",
      "        [0.6090, 0.3910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.4501, 0.5499],\n",
      "        [0.4501, 0.5499],\n",
      "        [0.5283, 0.4717],\n",
      "        [0.6086, 0.3914]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.4441, 0.5559],\n",
      "        [0.4441, 0.5559],\n",
      "        [0.5174, 0.4826],\n",
      "        [0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.4344, 0.5656],\n",
      "        [0.4344, 0.5656],\n",
      "        [0.5017, 0.4983],\n",
      "        [0.5980, 0.4020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 1], device='cuda:0')\n",
      "Epoch [30/100], Loss: 0.7204\n",
      "tensor([[0.4233, 0.5767],\n",
      "        [0.4233, 0.5767],\n",
      "        [0.4815, 0.5185],\n",
      "        [0.5893, 0.4107]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.4110, 0.5890],\n",
      "        [0.4110, 0.5890],\n",
      "        [0.4576, 0.5424],\n",
      "        [0.5781, 0.4219]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.3955, 0.6045],\n",
      "        [0.3955, 0.6045],\n",
      "        [0.4302, 0.5698],\n",
      "        [0.5664, 0.4336]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.3809, 0.6191],\n",
      "        [0.3809, 0.6191],\n",
      "        [0.4074, 0.5926],\n",
      "        [0.5583, 0.4417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.3753, 0.6247],\n",
      "        [0.3753, 0.6247],\n",
      "        [0.3994, 0.6006],\n",
      "        [0.5558, 0.4442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.3692, 0.6308],\n",
      "        [0.3692, 0.6308],\n",
      "        [0.3907, 0.6093],\n",
      "        [0.5532, 0.4468]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.3661, 0.6339],\n",
      "        [0.3661, 0.6339],\n",
      "        [0.3860, 0.6140],\n",
      "        [0.5515, 0.4485]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.3701, 0.6299],\n",
      "        [0.3701, 0.6299],\n",
      "        [0.3929, 0.6071],\n",
      "        [0.5544, 0.4456]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.3759, 0.6241],\n",
      "        [0.3759, 0.6241],\n",
      "        [0.4019, 0.5981],\n",
      "        [0.5576, 0.4424]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.3833, 0.6167],\n",
      "        [0.3833, 0.6167],\n",
      "        [0.4126, 0.5874],\n",
      "        [0.5608, 0.4392]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1], device='cuda:0')\n",
      "Epoch [40/100], Loss: 0.7934\n",
      "tensor([[0.3944, 0.6056],\n",
      "        [0.3944, 0.6056],\n",
      "        [0.4280, 0.5720],\n",
      "        [0.5649, 0.4351]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.3995, 0.6005],\n",
      "        [0.3995, 0.6005],\n",
      "        [0.4331, 0.5669],\n",
      "        [0.5647, 0.4353]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor([[0.4051, 0.5949],\n",
      "        [0.4051, 0.5949],\n",
      "        [0.4366, 0.5634],\n",
      "        [0.5625, 0.4375]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.4139, 0.5861],\n",
      "        [0.4139, 0.5861],\n",
      "        [0.4445, 0.5555],\n",
      "        [0.5615, 0.4385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.4250, 0.5750],\n",
      "        [0.4250, 0.5750],\n",
      "        [0.4558, 0.5442],\n",
      "        [0.5616, 0.4384]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.4378, 0.5622],\n",
      "        [0.4378, 0.5622],\n",
      "        [0.4695, 0.5305],\n",
      "        [0.5624, 0.4376]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.4461, 0.5539],\n",
      "        [0.4461, 0.5539],\n",
      "        [0.4776, 0.5224],\n",
      "        [0.5622, 0.4378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.4524, 0.5476],\n",
      "        [0.4524, 0.5476],\n",
      "        [0.4835, 0.5165],\n",
      "        [0.5617, 0.4383]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.4590, 0.5410],\n",
      "        [0.4590, 0.5410],\n",
      "        [0.4897, 0.5103],\n",
      "        [0.5615, 0.4385]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.4618, 0.5382],\n",
      "        [0.4618, 0.5382],\n",
      "        [0.4892, 0.5108],\n",
      "        [0.5583, 0.4417]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 0], device='cuda:0')\n",
      "Epoch [50/100], Loss: 0.6821\n",
      "tensor([[0.4655, 0.5345],\n",
      "        [0.4655, 0.5345],\n",
      "        [0.4925, 0.5075],\n",
      "        [0.5578, 0.4422]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.4681, 0.5319],\n",
      "        [0.4681, 0.5319],\n",
      "        [0.4943, 0.5057],\n",
      "        [0.5570, 0.4430]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.4732, 0.5268],\n",
      "        [0.4732, 0.5268],\n",
      "        [0.5013, 0.4987],\n",
      "        [0.5590, 0.4410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor([[0.4779, 0.5221],\n",
      "        [0.4779, 0.5221],\n",
      "        [0.5059, 0.4941],\n",
      "        [0.5590, 0.4410]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.4828, 0.5172],\n",
      "        [0.4828, 0.5172],\n",
      "        [0.5109, 0.4891],\n",
      "        [0.5591, 0.4409]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.4878, 0.5122],\n",
      "        [0.4878, 0.5122],\n",
      "        [0.5161, 0.4839],\n",
      "        [0.5594, 0.4406]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.4921, 0.5079],\n",
      "        [0.4921, 0.5079],\n",
      "        [0.5221, 0.4779],\n",
      "        [0.5613, 0.4387]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.4981, 0.5019],\n",
      "        [0.4981, 0.5019],\n",
      "        [0.5323, 0.4677],\n",
      "        [0.5656, 0.4344]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.5045, 0.4955],\n",
      "        [0.5045, 0.4955],\n",
      "        [0.5443, 0.4557],\n",
      "        [0.5714, 0.4286]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.5108, 0.4892],\n",
      "        [0.5108, 0.4892],\n",
      "        [0.5557, 0.4443],\n",
      "        [0.5766, 0.4234]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "Epoch [60/100], Loss: 0.7220\n",
      "tensor([[0.5165, 0.4835],\n",
      "        [0.5165, 0.4835],\n",
      "        [0.5640, 0.4360],\n",
      "        [0.5795, 0.4205]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.5208, 0.4792],\n",
      "        [0.5208, 0.4792],\n",
      "        [0.5704, 0.4296],\n",
      "        [0.5817, 0.4183]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.5257, 0.4743],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5789, 0.4211],\n",
      "        [0.5856, 0.4144]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.5293, 0.4707],\n",
      "        [0.5293, 0.4707],\n",
      "        [0.5853, 0.4147],\n",
      "        [0.5886, 0.4114]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.5345, 0.4655],\n",
      "        [0.5345, 0.4655],\n",
      "        [0.5953, 0.4047],\n",
      "        [0.5937, 0.4063]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 1], device='cuda:0')\n",
      "tensor([[0.5380, 0.4620],\n",
      "        [0.5380, 0.4620],\n",
      "        [0.6007, 0.3993],\n",
      "        [0.5959, 0.4041]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.5426, 0.4574],\n",
      "        [0.5426, 0.4574],\n",
      "        [0.6075, 0.3925],\n",
      "        [0.5983, 0.4017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.5449, 0.4551],\n",
      "        [0.5449, 0.4551],\n",
      "        [0.6109, 0.3891],\n",
      "        [0.5996, 0.4004]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.5464, 0.4536],\n",
      "        [0.5464, 0.4536],\n",
      "        [0.6128, 0.3872],\n",
      "        [0.6002, 0.3998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.5481, 0.4519],\n",
      "        [0.5481, 0.4519],\n",
      "        [0.6150, 0.3850],\n",
      "        [0.6009, 0.3991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 0], device='cuda:0')\n",
      "Epoch [70/100], Loss: 0.6427\n",
      "tensor([[0.5504, 0.4496],\n",
      "        [0.5504, 0.4496],\n",
      "        [0.6195, 0.3805],\n",
      "        [0.6033, 0.3967]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.5545, 0.4455],\n",
      "        [0.5545, 0.4455],\n",
      "        [0.6275, 0.3725],\n",
      "        [0.6077, 0.3923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.5586, 0.4414],\n",
      "        [0.5586, 0.4414],\n",
      "        [0.6352, 0.3648],\n",
      "        [0.6117, 0.3883]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor([[0.5622, 0.4378],\n",
      "        [0.5622, 0.4378],\n",
      "        [0.6402, 0.3598],\n",
      "        [0.6135, 0.3865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.5648, 0.4352],\n",
      "        [0.5648, 0.4352],\n",
      "        [0.6435, 0.3565],\n",
      "        [0.6147, 0.3853]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor([[0.5669, 0.4331],\n",
      "        [0.5669, 0.4331],\n",
      "        [0.6446, 0.3554],\n",
      "        [0.6139, 0.3861]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.5708, 0.4292],\n",
      "        [0.5708, 0.4292],\n",
      "        [0.6496, 0.3504],\n",
      "        [0.6155, 0.3845]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.5724, 0.4276],\n",
      "        [0.5724, 0.4276],\n",
      "        [0.6513, 0.3487],\n",
      "        [0.6158, 0.3842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.5725, 0.4275],\n",
      "        [0.5725, 0.4275],\n",
      "        [0.6494, 0.3506],\n",
      "        [0.6138, 0.3862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.5708, 0.4292],\n",
      "        [0.5708, 0.4292],\n",
      "        [0.6449, 0.3551],\n",
      "        [0.6109, 0.3891]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "Epoch [80/100], Loss: 0.6346\n",
      "tensor([[0.5701, 0.4299],\n",
      "        [0.5701, 0.4299],\n",
      "        [0.6433, 0.3567],\n",
      "        [0.6100, 0.3900]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor([[0.5693, 0.4307],\n",
      "        [0.5693, 0.4307],\n",
      "        [0.6401, 0.3599],\n",
      "        [0.6074, 0.3926]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 0], device='cuda:0')\n",
      "tensor([[0.5694, 0.4306],\n",
      "        [0.5694, 0.4306],\n",
      "        [0.6396, 0.3604],\n",
      "        [0.6069, 0.3931]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.5703, 0.4297],\n",
      "        [0.5703, 0.4297],\n",
      "        [0.6415, 0.3585],\n",
      "        [0.6080, 0.3920]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.5702, 0.4298],\n",
      "        [0.5702, 0.4298],\n",
      "        [0.6419, 0.3581],\n",
      "        [0.6085, 0.3915]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.5695, 0.4305],\n",
      "        [0.5695, 0.4305],\n",
      "        [0.6411, 0.3589],\n",
      "        [0.6084, 0.3916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1], device='cuda:0')\n",
      "tensor([[0.5692, 0.4308],\n",
      "        [0.5692, 0.4308],\n",
      "        [0.6406, 0.3594],\n",
      "        [0.6082, 0.3918]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.5671, 0.4329],\n",
      "        [0.5671, 0.4329],\n",
      "        [0.6376, 0.3624],\n",
      "        [0.6070, 0.3930]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.5641, 0.4359],\n",
      "        [0.5641, 0.4359],\n",
      "        [0.6317, 0.3683],\n",
      "        [0.6038, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.5622, 0.4378],\n",
      "        [0.5622, 0.4378],\n",
      "        [0.6287, 0.3713],\n",
      "        [0.6025, 0.3975]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1], device='cuda:0')\n",
      "Epoch [90/100], Loss: 0.6598\n",
      "tensor([[0.5618, 0.4382],\n",
      "        [0.5618, 0.4382],\n",
      "        [0.6277, 0.3723],\n",
      "        [0.6018, 0.3982]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.5607, 0.4393],\n",
      "        [0.5607, 0.4393],\n",
      "        [0.6256, 0.3744],\n",
      "        [0.6007, 0.3993]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0], device='cuda:0')\n",
      "tensor([[0.5591, 0.4409],\n",
      "        [0.5591, 0.4409],\n",
      "        [0.6226, 0.3774],\n",
      "        [0.5992, 0.4008]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[0.5570, 0.4430],\n",
      "        [0.5570, 0.4430],\n",
      "        [0.6189, 0.3811],\n",
      "        [0.5973, 0.4027]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor([[0.5550, 0.4450],\n",
      "        [0.5550, 0.4450],\n",
      "        [0.6137, 0.3863],\n",
      "        [0.5939, 0.4061]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.5526, 0.4474],\n",
      "        [0.5526, 0.4474],\n",
      "        [0.6082, 0.3918],\n",
      "        [0.5905, 0.4095]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.5512, 0.4488],\n",
      "        [0.5512, 0.4488],\n",
      "        [0.6055, 0.3945],\n",
      "        [0.5891, 0.4109]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0], device='cuda:0')\n",
      "tensor([[0.5503, 0.4497],\n",
      "        [0.5503, 0.4497],\n",
      "        [0.6035, 0.3965],\n",
      "        [0.5879, 0.4121]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([[0.5513, 0.4487],\n",
      "        [0.5513, 0.4487],\n",
      "        [0.6054, 0.3946],\n",
      "        [0.5890, 0.4110]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 1], device='cuda:0')\n",
      "tensor([[0.5511, 0.4489],\n",
      "        [0.5511, 0.4489],\n",
      "        [0.6041, 0.3959],\n",
      "        [0.5879, 0.4121]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "Epoch [100/100], Loss: 0.7176\n"
     ]
    }
   ],
   "source": [
    "class DEHNNLayer(nn.Module):\n",
    "    def __init__(self, node_in_features, edge_in_features):\n",
    "        super(DEHNNLayer, self).__init__()\n",
    "        self.node_mlp1 = nn.Linear(edge_in_features, edge_in_features)\n",
    "        # self.edge_mlp1 = nn.Linear(node_in_features, edge_in_features)\n",
    "        self.edge_mlp2 = nn.Linear(node_in_features, node_in_features)\n",
    "        self.edge_mlp3 = nn.Linear(2 * node_in_features, 2 * node_in_features)  # No compression, keeps 2 * out_features\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        # Node update\n",
    "        updated_node_features = {}\n",
    "        for node in hypergraph.nodes:\n",
    "            incident_edges = hypergraph.get_incident_edges(node)\n",
    "            agg_features = torch.sum(torch.stack([self.node_mlp1(edge_features[edge]) for edge in incident_edges]), dim=0)\n",
    "            updated_node_features[node] = agg_features\n",
    "\n",
    "        # Edge update\n",
    "        updated_edge_features = {}\n",
    "        for edge in hypergraph.edges:\n",
    "            driver, sinks = hypergraph.get_driver_and_sinks(edge)\n",
    "            sink_agg = torch.sum(torch.stack([self.edge_mlp2(node_features[sink]) for sink in sinks]), dim=0)\n",
    "            concatenated = torch.cat([node_features[driver], sink_agg])\n",
    "            updated_edge_features[edge] = self.edge_mlp3(concatenated)\n",
    "\n",
    "        return updated_node_features, updated_edge_features\n",
    "\n",
    "\n",
    "class DEHNN(nn.Module):\n",
    "    def __init__(self, num_layers, node_in_features, edge_in_features):\n",
    "        super(DEHNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Create multiple layers for DEHNN\n",
    "        for i in range(num_layers):\n",
    "            print(node_in_features, edge_in_features)\n",
    "            # in_features = node_in_features if i > 0 else edge_in_features\n",
    "            self.layers.append(DEHNNLayer(node_in_features, edge_in_features))\n",
    "            node_in_features, edge_in_features = edge_in_features, node_in_features\n",
    "            edge_in_features *= 2\n",
    "\n",
    "        print(node_in_features, edge_in_features)\n",
    "        edge_in_features  = int(edge_in_features / 2)\n",
    "        # Final output layer for node classification (binary classification for congestion)\n",
    "        self.output_layer = nn.Linear(node_in_features, 2)  # Output 2 classes: congested or not congested\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        # Pass through each layer\n",
    "        for layer in self.layers:\n",
    "            node_features, edge_features = layer(node_features, edge_features, hypergraph)\n",
    "        \n",
    "        # Output prediction for nodes\n",
    "        final_node_features = torch.stack([node_features[node] for node in hypergraph.nodes], dim=0)\n",
    "        # print(node_features[0].shape)\n",
    "        # print(final_node_features.shape)\n",
    "        output = self.output_layer(final_node_features)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Example hypergraph representation class (simplified)\n",
    "class Hypergraph:\n",
    "    def __init__(self, nodes, edges, driver_sink_map):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.driver_sink_map = driver_sink_map\n",
    "\n",
    "    def get_incident_edges(self, node):\n",
    "        return [edge for edge in self.edges if node in self.driver_sink_map[edge][1] or node == self.driver_sink_map[edge][0]]\n",
    "\n",
    "    def get_driver_and_sinks(self, edge):\n",
    "        return self.driver_sink_map[edge]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dummy data for illustration\n",
    "nodes = [0, 1, 2, 3]\n",
    "edges = [0, 1]\n",
    "driver_sink_map = {0: (0, [1, 2]), 1: (2, [3])}  # edge 0: driver is 0, sinks are 1, 2\n",
    "node_features = {i: torch.randn(10).to(device) for i in nodes}\n",
    "edge_features = {i: torch.randn(15).to(device) for i in edges}\n",
    "\n",
    "hypergraph = Hypergraph(nodes, edges, driver_sink_map)\n",
    "\n",
    "# Initialize DE-HNN model\n",
    "model = DEHNN(num_layers=3, node_in_features=10, edge_in_features=15).to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "\n",
    "# Training Loop (example)\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(node_features, edge_features, hypergraph)\n",
    "    print(output)\n",
    "    \n",
    "    # Dummy target for illustration (binary labels for each node: 0 for not congested, 1 for congested)\n",
    "    target = torch.randint(0, 2, (len(nodes),)).to(device)\n",
    "    \n",
    "    print(target)\n",
    "    # Compute loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
