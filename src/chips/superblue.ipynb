{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rebal\\capstone-project\\capstone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypergraph:\n",
    "    def __init__(self, incidence_matrix, driver_matrix, sink_matrix, vn_matrix):\n",
    "        self.incidence_matrix = incidence_matrix\n",
    "        self.driver_matrix = driver_matrix\n",
    "        self.sink_matrix = sink_matrix\n",
    "        self.vn_matrix = vn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_chip(i):\n",
    "    path = '../../data/chips/2023-03-06_data/'\n",
    "\n",
    "    with open(path + f'{i}.bipartite.pkl', 'rb') as f:\n",
    "        bipartite = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.degree.pkl', 'rb') as f:\n",
    "        degree = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.eigen.10.pkl', 'rb') as f:\n",
    "        eigen = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.global_information.pkl', 'rb') as f:\n",
    "        global_info = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.metis_part_dict.pkl', 'rb') as f:\n",
    "        metis = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.net_demand_capacity.pkl', 'rb') as f:\n",
    "        net_demand_capacity = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.net_features.pkl', 'rb') as f:\n",
    "        net_feats = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.net_hpwl.pkl', 'rb') as f:\n",
    "        hpwl = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.nn_conn.pkl', 'rb') as f:\n",
    "        nn_conn = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.node_features.pkl', 'rb') as f:\n",
    "        node_feats = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.pl_fix_part_dict.pkl', 'rb') as f:\n",
    "        pl = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.star.pkl', 'rb') as f:\n",
    "        star = pickle.load(f)\n",
    "\n",
    "    with open(path + f'{i}.targets.pkl', 'rb') as f:\n",
    "        targets = pickle.load(f)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    incidence_matrix = torch.sparse_coo_tensor(torch.tensor(np.array([bipartite['instance_idx'], bipartite['net_idx']])), torch.ones(bipartite['edge_dir'].shape), dtype=torch.float).to(device)\n",
    "\n",
    "    driver_idx = bipartite['edge_dir'] == 1\n",
    "    driver_row = bipartite['instance_idx'][driver_idx]\n",
    "    driver_col = bipartite['net_idx'][driver_idx]\n",
    "    driver_dir = bipartite['edge_dir'][driver_idx]\n",
    "    driver_matrix = torch.sparse_coo_tensor(torch.tensor(np.array([driver_col, driver_row])), torch.ones(driver_dir.shape), dtype=torch.float).to(device)\n",
    "\n",
    "    sink_idx = bipartite['edge_dir'] == 0\n",
    "    sink_row = bipartite['instance_idx'][sink_idx]\n",
    "    sink_col = bipartite['net_idx'][sink_idx]\n",
    "    sink_dir = bipartite['edge_dir'][sink_idx]\n",
    "    sink_matrix = torch.sparse_coo_tensor(torch.tensor(np.array([sink_col, sink_row])), torch.ones(sink_dir.shape), dtype=torch.float).to(device)\n",
    "\n",
    "    num_nodes = node_feats['num_instances']\n",
    "    node_features = node_feats['instance_features']\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float).to(device)\n",
    "    evects = torch.tensor(eigen['evects'][:num_nodes]).to(device)\n",
    "    node_features = torch.cat([node_features, evects], dim=1)\n",
    "\n",
    "    net_features = net_feats['instance_features'][:,:10]\n",
    "    edge_features = torch.tensor(net_features, dtype=torch.float).to(device)\n",
    "\n",
    "    num_nodes, num_node_features = node_features.shape\n",
    "    num_edges, num_edge_features = edge_features.shape\n",
    "    demand = targets['demand'].reshape(num_nodes,1)\n",
    "    demand = torch.tensor(demand, dtype=torch.float).to(device)\n",
    "    wire = hpwl['hpwl'].reshape(num_edges,1)\n",
    "    wire = torch.tensor(wire, dtype=torch.float).to(device)\n",
    "\n",
    "    vn_row = []\n",
    "    vn_col = []\n",
    "\n",
    "    for k, v in metis.items():\n",
    "        if k < num_nodes:\n",
    "            vn_row.append(v)\n",
    "            vn_col.append(k)\n",
    "\n",
    "    vn_matrix = torch.sparse_coo_tensor(torch.tensor(np.array([vn_row, vn_col])), torch.ones(len(vn_row)), dtype=torch.float).to(device)\n",
    "\n",
    "    num_vn = vn_matrix.shape[0]\n",
    "    num_vn_features = num_node_features\n",
    "    vn_features = torch.zeros((num_vn, num_vn_features), dtype=torch.float).to(device)\n",
    "    super_vn_features = torch.zeros(num_vn_features, dtype=torch.float).to(device)\n",
    "\n",
    "    hypergraph = Hypergraph(incidence_matrix, driver_matrix, sink_matrix, vn_matrix)\n",
    "\n",
    "    return node_features, edge_features, vn_features, super_vn_features, hypergraph, wire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features, edge_features, vn_features, super_vn_features, hypergraph, wire = open_chip(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23.7762],\n",
       "        [ 8.6795],\n",
       "        [ 8.6795],\n",
       "        ...,\n",
       "        [17.7001],\n",
       "        [14.6551],\n",
       "        [13.1548]], device='cuda:0')"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.1809684 ,  8.6794801 ,  8.6794801 , ..., 13.75238065,\n",
       "       14.2644426 , 13.06676193])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpwl['hpwl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([495234, 16])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 797938\n",
      "3 797938\n",
      "5 951166\n",
      "8 951166\n",
      "10 901254\n",
      "13 901254\n",
      "16 556885\n",
      "21 727341\n",
      "24 727341\n",
      "27 998122\n",
      "30 998122\n",
      "33 1319052\n",
      "36 1319052\n",
      "39 502057\n",
      "40 459495\n",
      "42 459495\n",
      "44 495234\n",
      "47 495234\n",
      "50 810812\n",
      "52 810812\n",
      "56 923355\n",
      "58 923355\n",
      "62 604921\n",
      "64 604921\n",
      "68 671284\n",
      "70 671284\n"
     ]
    }
   ],
   "source": [
    "for i in range(71):\n",
    "    try:\n",
    "        with open(path + f'{i}.node_features.pkl', 'rb') as f:\n",
    "            nf = pickle.load(f)\n",
    "            print(i, nf['num_instances'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/chips/2023-03-06_data/'\n",
    "\n",
    "with open(path + '0.bipartite.pkl', 'rb') as f:\n",
    "    bipartite = pickle.load(f)\n",
    "\n",
    "with open(path + '0.degree.pkl', 'rb') as f:\n",
    "    degree = pickle.load(f)\n",
    "\n",
    "with open(path + '0.eigen.10.pkl', 'rb') as f:\n",
    "    eigen = pickle.load(f)\n",
    "\n",
    "with open(path + '0.global_information.pkl', 'rb') as f:\n",
    "    global_info = pickle.load(f)\n",
    "\n",
    "with open(path + '0.metis_part_dict.pkl', 'rb') as f:\n",
    "    metis = pickle.load(f)\n",
    "\n",
    "with open(path + '0.net_demand_capacity.pkl', 'rb') as f:\n",
    "    net_demand_capacity = pickle.load(f)\n",
    "\n",
    "with open(path + '0.net_features.pkl', 'rb') as f:\n",
    "    net_feats = pickle.load(f)\n",
    "\n",
    "with open(path + '0.net_hpwl.pkl', 'rb') as f:\n",
    "    hpwl = pickle.load(f)\n",
    "\n",
    "with open(path + '0.nn_conn.pkl', 'rb') as f:\n",
    "    nn_conn = pickle.load(f)\n",
    "\n",
    "with open(path + '0.node_features.pkl', 'rb') as f:\n",
    "    node_feats = pickle.load(f)\n",
    "\n",
    "with open(path + '0.pl_fix_part_dict.pkl', 'rb') as f:\n",
    "    pl = pickle.load(f)\n",
    "\n",
    "with open(path + '0.star.pkl', 'rb') as f:\n",
    "    star = pickle.load(f)\n",
    "\n",
    "with open(path + '0.targets.pkl', 'rb') as f:\n",
    "    targets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "incidence_matrix = torch.sparse_coo_tensor(torch.tensor(np.array([bipartite['instance_idx'], bipartite['net_idx']])), torch.ones(bipartite['edge_dir'].shape), dtype=torch.float).to(device)\n",
    "\n",
    "driver_idx = bipartite['edge_dir'] == 1\n",
    "driver_row = bipartite['instance_idx'][driver_idx]\n",
    "driver_col = bipartite['net_idx'][driver_idx]\n",
    "driver_dir = bipartite['edge_dir'][driver_idx]\n",
    "driver_matrix = torch.sparse_coo_tensor(torch.tensor(np.array([driver_col, driver_row])), torch.ones(driver_dir.shape), dtype=torch.float).to(device)\n",
    "\n",
    "sink_idx = bipartite['edge_dir'] == 0\n",
    "sink_row = bipartite['instance_idx'][sink_idx]\n",
    "sink_col = bipartite['net_idx'][sink_idx]\n",
    "sink_dir = bipartite['edge_dir'][sink_idx]\n",
    "sink_matrix = torch.sparse_coo_tensor(torch.tensor(np.array([sink_col, sink_row])), torch.ones(sink_dir.shape), dtype=torch.float).to(device)\n",
    "\n",
    "num_nodes = node_feats['num_instances']\n",
    "node_features = node_feats['instance_features']\n",
    "node_features = torch.tensor(node_features, dtype=torch.float).to(device)\n",
    "evects = torch.tensor(eigen['evects'][:num_nodes]).to(device)\n",
    "node_features = torch.cat([node_features, evects], dim=1)\n",
    "\n",
    "net_features = net_feats['instance_features']\n",
    "edge_features = torch.tensor(net_features, dtype=torch.float).to(device)\n",
    "\n",
    "num_nodes, num_node_features = node_features.shape\n",
    "num_edges, num_edge_features = edge_features.shape\n",
    "demand = targets['demand'].reshape(num_nodes,1)\n",
    "demand = torch.tensor(demand, dtype=torch.float).to(device)\n",
    "\n",
    "vn_row = []\n",
    "vn_col = []\n",
    "\n",
    "for k, v in metis.items():\n",
    "    if k < num_nodes:\n",
    "        vn_row.append(v)\n",
    "        vn_col.append(k)\n",
    "\n",
    "vn_matrix = torch.sparse_coo_tensor(torch.tensor(np.array([vn_row, vn_col])), torch.ones(len(vn_row)), dtype=torch.float).to(device)\n",
    "\n",
    "num_vn = vn_matrix.shape[0]\n",
    "num_vn_features = num_node_features\n",
    "vn_features = torch.zeros((num_vn, num_vn_features), dtype=torch.float).to(device)\n",
    "super_vn_features = torch.zeros(num_vn_features, dtype=torch.float).to(device)\n",
    "\n",
    "hypergraph = Hypergraph(incidence_matrix, driver_matrix, sink_matrix, vn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEHNNLayer(nn.Module):\n",
    "    def __init__(self, node_in_features, edge_in_features, vn_features, hidden_features):\n",
    "        super(DEHNNLayer, self).__init__()\n",
    "        self.node_mlp1 = nn.Sequential(nn.Linear(edge_in_features, hidden_features),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_features, edge_in_features))\n",
    "        \n",
    "        self.edge_mlp2 = nn.Sequential(nn.Linear(node_in_features, hidden_features),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_features, node_in_features))\n",
    "        \n",
    "        self.edge_mlp3 = nn.Sequential(nn.Linear(2 * node_in_features, hidden_features),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_features, 2 * node_in_features))\n",
    "\n",
    "        self.node_to_virtual_mlp = nn.Sequential(nn.Linear(node_in_features, hidden_features),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_features, vn_features))\n",
    "        \n",
    "        self.virtual_to_higher_virtual_mlp = nn.Sequential(nn.Linear(vn_features, hidden_features),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_features, vn_features))\n",
    "        \n",
    "        self.higher_virtual_to_virtual_mlp = nn.Sequential(nn.Linear(vn_features, hidden_features),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_features, vn_features))\n",
    "        \n",
    "        self.virtual_to_node_mlp = nn.Sequential(nn.Linear(vn_features, hidden_features),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_features, edge_in_features))\n",
    "\n",
    "\n",
    "    def forward(self, node_features, edge_features, vn_features, super_vn_features, hypergraph):\n",
    "\n",
    "        # Node Update\n",
    "        transformed_edge_features = self.node_mlp1(edge_features)\n",
    "        updated_node_features = torch.matmul(hypergraph.incidence_matrix, transformed_edge_features)\n",
    "\n",
    "        # Edge Update\n",
    "        transformed_node_features = self.edge_mlp2(node_features)\n",
    "        driver_features = torch.matmul(hypergraph.driver_matrix, transformed_node_features)\n",
    "        sink_features = torch.matmul(hypergraph.sink_matrix, transformed_node_features)\n",
    "        updated_edge_features = torch.cat([driver_features, sink_features], dim=1)\n",
    "        updated_edge_features = self.edge_mlp3(updated_edge_features)\n",
    "        \n",
    "        # First Level VN Update\n",
    "        node_to_virtual_features = self.node_to_virtual_mlp(node_features)\n",
    "        updated_vn_features = torch.matmul(hypergraph.vn_matrix, node_to_virtual_features)\n",
    "        updated_vn_features += self.higher_virtual_to_virtual_mlp(super_vn_features)\n",
    "\n",
    "        # Top Level VN Update\n",
    "        virtual_to_higher_virtual_features = self.virtual_to_higher_virtual_mlp(vn_features)\n",
    "        updated_super_vn_features = torch.sum(virtual_to_higher_virtual_features, dim=0)\n",
    "\n",
    "        # VN to node update\n",
    "        virtual_to_node_features = self.virtual_to_node_mlp(vn_features)\n",
    "        propagated_features = torch.matmul(hypergraph.vn_matrix.T, virtual_to_node_features)\n",
    "        updated_node_features += propagated_features\n",
    "\n",
    "        return updated_node_features, updated_edge_features, updated_vn_features, updated_super_vn_features\n",
    "\n",
    "\n",
    "class DEHNN(nn.Module):\n",
    "    def __init__(self, num_layers, node_in_features, edge_in_features, hidden_features=24):\n",
    "        super(DEHNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Create multiple layers for DEHNN\n",
    "        vn_in_features = node_in_features\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DEHNNLayer(node_in_features, edge_in_features, vn_in_features, hidden_features))\n",
    "            node_in_features, edge_in_features = edge_in_features, node_in_features\n",
    "            edge_in_features *= 2\n",
    "\n",
    "        # edge_in_features = int(edge_in_features/2)\n",
    "        # self.output_layer = nn.Sequential(nn.Linear(node_in_features, hidden_features),\n",
    "        #                                nn.ReLU(),\n",
    "        #                                nn.Linear(hidden_features, 1))\n",
    "        \n",
    "        self.output_layer = nn.Sequential(nn.Linear(edge_in_features, hidden_features),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_features, 1))\n",
    "\n",
    "    def forward(self, node_features, edge_features, vn_features, super_vn_features, hypergraph):\n",
    "        # Pass through each layer\n",
    "        for layer in self.layers:\n",
    "            node_features, edge_features, vn_features, super_vn_features = layer(node_features, edge_features, vn_features, super_vn_features, hypergraph)\n",
    "        \n",
    "        # Output prediction for nodes\n",
    "        # output = self.output_layer(node_features)\n",
    "        output = self.output_layer(edge_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features = 16\n",
    "num_edge_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "train_idx = [16, 39, 40, 62, 68]\n",
    "valid_idx = 44\n",
    "\n",
    "node_features, edge_features, vn_features, super_vn_features, hypergraph, wire = open_chip(40)\n",
    "valid_node_features, valid_edge_features, valid_vn_features, valid_super_vn_features, valid_hypergraph, valid_wire = open_chip(44)\n",
    "\n",
    "# Initialize DE-HNN model\n",
    "# model = DEHNN(num_layers=2, node_in_features=num_node_features, edge_in_features=num_edge_features).to(device)\n",
    "model = torch.load('cross_design_hpwl_2.pt').to(device)\n",
    "epochs = 50\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Training Loss: 18.9296, Validation Loss: 9.3325\n",
      "Epoch [20/50], Training Loss: 24.9845, Validation Loss: 7.1899\n",
      "Epoch [30/50], Training Loss: 49.1969, Validation Loss: 8.5209\n",
      "Epoch [40/50], Training Loss: 9.6039, Validation Loss: 6.1355\n",
      "Epoch [50/50], Training Loss: 12.3788, Validation Loss: 7.1596\n"
     ]
    }
   ],
   "source": [
    "# Training Loop (example)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i in train_idx:\n",
    "        node_features, edge_features, vn_features, super_vn_features, hypergraph, wire = open_chip(i)\n",
    "\n",
    "        output = model(node_features, edge_features, vn_features, super_vn_features, hypergraph)\n",
    "\n",
    "        loss = criterion(output, wire)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    if epoch % 10 == 9:\n",
    "        model.eval()\n",
    "        output = model(valid_node_features, valid_edge_features, valid_vn_features, valid_super_vn_features, valid_hypergraph)\n",
    "        valid_loss = criterion(output, valid_wire)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {valid_loss.item():.4f}')\n",
    "        # print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'cross_design_hpwl_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features, edge_features, vn_features, super_vn_features, hypergraph, demand = open_chip(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(node_features, edge_features, vn_features, super_vn_features, hypergraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.29208525e+04, 1.17586937e+01, 1.17586937e+01, ...,\n",
       "       1.49999514e+01, 1.49969635e+01, 1.49731808e+01], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = output.detach().cpu().numpy()[:,0]\n",
    "real = demand.detach().cpu().numpy()[:,0]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0278354"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(pred - real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.997997"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((pred - real) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
